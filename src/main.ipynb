{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import appsettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER = appsettings.SERVER\n",
    "DATABASE = appsettings.DATABASE\n",
    "SCHEMA = 'SalesLT'\n",
    "UID = appsettings.UID\n",
    "PASSWORD = appsettings.PASSWORD\n",
    "DRIVER = '{ODBC Driver 18 for SQL Server}'\n",
    "ROWS = 10000\n",
    "LEVEL = 't'\n",
    "TABLE = 'Product'\n",
    "ASSOCIATIONS = False\n",
    "OPEN_BROWSER = False\n",
    "INTERACTIVE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import sweetviz as sv\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/58440480/connect-to-azure-sql-in-python-with-mfa-active-directory-interactive-authenticat\n",
    "\n",
    "def create_connection():\n",
    "    if INTERACTIVE:\n",
    "        return pyodbc.connect('DRIVER='+DRIVER+';SERVER='+SERVER+';PORT=1433;DATABASE='+DATABASE+';AUTHENTICATION=ActiveDirectoryInteractive')\n",
    "    else:\n",
    "        return pyodbc.connect('DRIVER='+DRIVER+';SERVER='+SERVER+';PORT=1433;DATABASE='+DATABASE+';UID='+UID+';PWD='+ PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = []\n",
    "\n",
    "if(LEVEL=='s'):\n",
    "    with create_connection() as conn1:\n",
    "        with conn1.cursor() as schema_cur:\n",
    "            schema_cur.execute(f\"SELECT TABLE_NAME FROM [{DATABASE}].INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE='BASE TABLE' AND TABLE_SCHEMA='{SCHEMA}'\")\n",
    "            schema_row = schema_cur.fetchone()\n",
    "            while schema_row:\n",
    "                table_name = str(schema_row[0]) \n",
    "                with create_connection() as conn2:\n",
    "                    with conn2.cursor() as space_cur:\n",
    "                        space_cur.execute(f\"EXEC sp_spaceused N'{SCHEMA}.{table_name}'\") # result columns: name rows reserved data index_size unused\n",
    "                        space_row = space_cur.fetchone()\n",
    "                        rows = str(space_row[1])\n",
    "                        data = str(space_row[3]) \n",
    "                        index_size = str(space_row[4]) \n",
    "                        tables.append((f'{SCHEMA}.{table_name}',rows,data,index_size))\n",
    "                schema_row = schema_cur.fetchone()\n",
    "else:\n",
    "    with create_connection() as conn2:\n",
    "        with conn2.cursor() as space_cur:\n",
    "            space_cur.execute(f\"EXEC sp_spaceused N'{SCHEMA}.{TABLE}'\") # result columns: name rows reserved data index_size unused\n",
    "            space_row = space_cur.fetchone()\n",
    "            rows = str(space_row[1])\n",
    "            data = str(space_row[3]) \n",
    "            index_size = str(space_row[4]) \n",
    "            tables.append((f'{SCHEMA}.{TABLE}',rows,data,index_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('obj'):\n",
    "    os.mkdir('obj')\n",
    "\n",
    "info_arr = []\n",
    "\n",
    "for (table_name,rows,data,index_size) in tables:\n",
    "    print(f'Loading {table_name}...')\n",
    "\n",
    "    sheet_name = table_name.split('.')[-1]\n",
    "\n",
    "    sample_query = f'SELECT * FROM {table_name} TABLESAMPLE ({ROWS} ROWS)'\n",
    "    with create_connection() as conn3:\n",
    "        sample_cur = conn3.cursor().execute(sample_query)\n",
    "        sample_df = pd.DataFrame.from_records(\n",
    "            iter(sample_cur), columns=[x[0] for x in sample_cur.description])\n",
    "\n",
    "        info_arr.append([table_name,rows,data,index_size, len(sample_df)])\n",
    "\n",
    "        analysis = sv.analyze(sample_df, pairwise_analysis=(\"on\" if ASSOCIATIONS else \"off\"))\n",
    "        analysis.show_html(f'obj/{sheet_name}.html', open_browser=OPEN_BROWSER)\n",
    "\n",
    "    del sample_df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_info_df = pd.DataFrame(info_arr, columns = ['NAME','TABLE ROWS','DATA','INDEX SIZE','SAMPLE ROWS'])\n",
    "excel_writer = pd.ExcelWriter(f'obj/{SCHEMA}_EDA_INFO.xlsx', engine='xlsxwriter')\n",
    "eda_info_df.to_excel(excel_writer, sheet_name=SCHEMA, index=False)\n",
    "worksheet = excel_writer.sheets[SCHEMA]\n",
    "for idx, col in enumerate(eda_info_df):  # Loop through all columns\n",
    "    series = eda_info_df[col]\n",
    "    max_len = max((\n",
    "        series.astype(str).str.len().max(),  # Len of largest item\n",
    "        len(str(series.name))  # Len of column name/header\n",
    "        )) + 9  # Adding a little extra space\n",
    "    worksheet.set_column(idx, idx, max_len)  # Set column width\n",
    "excel_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datafabric-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06fd14dd0464832b5d48561070e33a75f30377a884f82ade456f68b55551612f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
